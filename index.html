<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus, a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body, td, th, tr, p, a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        .hp-photo {
            width: 240px;
            height: 240px;
            border-radius: 240px;
            -webkit-border-radius: 240px;
            -moz-border-radius: 240px;
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 24px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }
    </style>

    <title>Qingyong Hu | University of Oxford</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="./imgs/oxford_icon.jpg">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody>
    <tr>
        <td>


            <!--SECTION 1 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="68%" valign="middle">
                        <p align="center">
                            <name>Qingyong Hu</name>
                        </p>
                        <p align="justify">I am a D.Phil student (Oct 2018 - ) in the <a href="http://www.cs.ox.ac.uk/">
                            Department of Computer Science</a> at the <a href="http://www.ox.ac.uk/">University of
                            Oxford</a>, supervised by <a href="https://www.cs.ox.ac.uk/people/niki.trigoni/"> Profs.
                            Niki Trigoni</a> and <a href="https://www.cs.ox.ac.uk/people/andrew.markham/">Andrew
                            Markham</a>. Prior to Oxford, I obtained my M.Eng degree and B.Eng degree from China and
                            supervised by <a href="http://yulanguo.me/">Profs. Yulan Guo</a>.

                            <!--                    </br></br>-->
                            <!--                    In this summer (July - Oct 2019), I was a research intern at the Augumented Reality team of <a href="http://www.a9.com/">Amazon</a> (Palo Alto, CA).-->
                            <!--                    In my M.Phil study, I interned at <a href="https://www.astri.org/">Hong Kong Applied Science and Technology Research Institute</a>.-->
                            <!--                    In my undergraduate study, I was an exchange student at <a href="http://www.upv.es/">Universitat Politècnica de València</a> (Valencia, Spain).-->

                            </br>
                        </p>
                        <p align="center">
                            <a href="mailto:qingyong.hu@cs.ox.ac.uk">Email</a> /
                            <a href="https://github.com/QingyongHu"> Github </a> /
                            <a href="https://www.zhihu.com/people/hu-qing-yong"> Blog </a> /
                            <a href="https://www.linkedin.com/in/qingyong-hu-b18061171/"> LinkedIn </a> /
                            <a href="https://twitter.com/home"> Twitter </a> /
                            <a href="https://scholar.google.com/citations?user=yboFNHEAAAAJ&hl=en">Google Scholar</a>


                        </p>
                    </td>
                    <td align="right"><img class="hp-photo" src="./imgs/photo.jpg" style="width: 240;"></td>
                </tr>
                </tbody>
            </table>

            <!--SECTION 2 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Research</heading>
                        <p align="justify">
                            I am interested in 3D computer vision, machine learning, and robotics. My research goal is
                            to build intelligent systems which are able to achieve an effective and efficient perception
                            and understanding of 3D scenes. In particular, my research focuses on large-scale point
                            cloud segmentation, dynamic point cloud processing, point cloud tracking, and local surface
                            matching. If you are interested in my research or have any use cases that you want to share,
                            feel free to <a href="mailto:huqingyong15@outlook.com">contact me</a>!
                            <!--</br></br>-->
                            <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!--SECTION 3 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>News</heading>
                        <p><strong>[2022.03.27]</strong> We are organizing 2nd Urban3D chanllenge in ECCV 2022!
                        <p><strong>[2022.03.02]</strong> Three papers (<a href="https://github.com/yifanzhang713/IA-SSD">IA-SSD</a>, <a href="https://github.com/jx-zhong-for-academic-purpose/Kinet">KiNet</a>, <a href="https://arxiv.org/abs/2203.09931">3DAC</a>) have been accepted to CVPR 2022!
                        <p><strong>[2022.02.05]</strong> Our <a
                                href="https://arxiv.org/abs/1911.11236">RandLA-Net </a> has been listed as 
                                <a
                                href="https://www.paperdigest.org/2022/02/most-influential-cvpr-papers-2022-02/">the most influential paper </a> in CVPR 2020!
                        <p><strong>[2022.01.04]</strong> Our <a
                                href="https://link.springer.com/article/10.1007/s11263-021-01554-9">SensatUrban dataset</a> has been extended to IJCV 2022!
                        <p><strong>[2020.12.03]</strong> Successfully defend <a
                                href="https://www.ox.ac.uk/students/academic/guidance/graduate/research/status/DPhil">DPhil confirmation</a> viva, examined by <a
                                href="https://scholar.google.com/citations?user=kPxa2w0AAAAJ&hl=en">Professor Philip Torr</a> and <a
                                href="https://www.cs.ox.ac.uk/people/marta.kwiatkowska/">Professor Marta Kwiatkowska</a>.</p>
                        <p><strong>[2021.11.23]</strong> Our <a
                                href="https://ieeexplore.ieee.org/document/9625976">VISO dataset</a> is accepted to IEEE TGRS 2021!
                        <p><strong>[2021.10.16]</strong> We are organizing <a
                                href="https://urban3dchallenge.github.io/">Urban3D Challenge</a> at ICCV 2021!
                        <p><strong>[2021.09.01]</strong> I was awarded as the <a
                                href="https://iccv2021.thecvf.com/outstanding-reviewers/">outstanding reviewers</a> for ICCV 2021 (top 5%)!
                        <p><strong>[2021.05.15]</strong> Our <a
                                href="https://arxiv.org/abs/1911.11236">RandLA-Net</a> has been extended to IEEE TPAMI 2021!
                        <p><strong>[2021.03.01]</strong> Two papers (<a href="http://arxiv.org/abs/2009.03137">SensatUrban</a>,
                            <a href="https://arxiv.org/abs/2011.12149">SpinNet</a>) are accepted to CVPR 2021!
                        <p><strong>[2021.01.30]</strong> One paper on efficient semantic segmentation is accepted to
                            ICASSP 2021!
                        <p><strong>[2020.10.15]</strong> our <a href="https://github.com/QingyongHu/RandLA-Net">RandLA-Net</a>
                            has been integrated to <a
                                    href="http://www.open3d.org/docs/release/python_api/open3d.ml.tf.models.RandLANet.html">Open3d</a>
                            as a standard module!
                        <p><strong>[2020.10.09]</strong> Invited to present our work on
                            <a href="https://wonderlandai.com/"> Wonderland AI Summit 2020</a>!
                        <p><strong>[2020.09.22]</strong> Invited to present on
                            <a href="https://sites.google.com/view/3d-dlad-v2-iv2020/schedule/"> 2nd Workshop on
                                3D-DLAD </a> at <a href="https://2020.ieee-iv.org/"> IEEE IV 2020</a>!
                        <p><strong>[2020.09.22]</strong> We will organize a tutorial of
                            <a href="http://3dlearning.cn/"> 3D Point Cloud Reconstruction and Segmentation </a>
                            in the <a href="http://3dv2020.dgcv.nii.ac.jp/"> 3DV 2020</a>!
                        <p><strong>[2020.09.08]</strong> Our <a href="http://arxiv.org/abs/2009.03137">SensatUrban</a>
                            (Urban-scale photogrammetric point cloud) dataset is on arXiv!</p>
                        <p><strong>[2020.08.05]</strong> One paper on <a href="https://arxiv.org/abs/2008.02312"> CNN
                            intepretation</a> is accepted as oral presentation at<a href="https://bmvc2020.github.io/">
                            BMVC2020</a>!</p>
                        <p><strong>[2020.06.23]</strong> Our <a href="https://arxiv.org/abs/1912.12033">point cloud
                            survey</a> paper has been accepted to IEEE TPAMI! </a></p>
                        <p><strong>[2020.03.08]</strong> Invited to present our <a
                                href="https://arxiv.org/abs/1911.11236">RandLA-Net</a>
                            and <a href="https://arxiv.org/abs/1906.01140">3D-BoNet</a> at Shenlan.
                            Here are the <a href="https://www.shenlanxueyuan.com/open/course/53">Video</a>
                            and
                            <a href="https://www.dropbox.com/s/80w91bqzfdl5vow/%E6%B7%B1%E8%93%9D%E5%AD%A6%E9%99%A2%E5%85%AC%E5%BC%80%E8%AF%BE_20200308.pdf?dl=0">Slides</a>.
                        </p>
                        <p><strong>[2020.02.24]</strong> Our <a href="https://arxiv.org/abs/1911.11236">RandLA-Net</a>
                            is accepted by CVPR2020! </a></p>
                        <p><strong>[2020.01.13]</strong> Successfully defend <a
                                href="https://www.ox.ac.uk/students/academic/guidance/graduate/research/status/DPhil?wssl=1">DPhil transfer</a> viva, examined by <a
                                href="http://www.cs.ox.ac.uk/people/alex.rogers/"> Profs. Alex Rogers</a> and <a
                                href="http://www.robots.ox.ac.uk/~victor/">Profs. Victor Adrian Prisacariu</a>.</p>
                        <p><strong>[2019.12.13]</strong> Attending and presenting our RandLA-Net on the <a
                                href="https://www.turing.ac.uk/collaborate-turing/data-study-groups">Turing Data Study
                            Group</a>!</p>
                        <p><strong>[2019.09.03]</strong> One paper on 3D instance segmentation is accepted as a
                            spotlight at <a href="https://nips.cc/Conferences/2019">NeurIPS 2019</a>. </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!--SECTION 4 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Publications / Preprints</heading>
                    </td>
                </tr>
                </tbody>
            </table>


            <!--SECTION 5 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/STPLS3D.gif" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2203.09065">
                            <papertitle>STPLS3D: A Large-Scale Synthetic and Real Aerial Photogrammetry 3D Point Cloud Dataset
                            </papertitle>
                        </a>
                            <br>M. Chen, <strong>Q. Hu*</strong>, T. Hugues, A. Feng, Y. Hou, K. McCullough, L. Soibelman<br>
                            (*indicates corresponding author)<br>
                            <em>Arixv 2022 </em><br>
                            <a href="https://arxiv.org/abs/2203.09065">Paper</a>/
                            <a href="https://www.youtube.com/watch?v=6wYWVo6Cmfs&t=20s">Demo</a> /
                            <a href="https://www.stpls3d.com/">Project Page</a> /
                            <a href="https://github.com/meidachen/STPLS3D"><font color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=meidachen&repo=STPLS3D&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px"
                                    height="20px"></iframe>
                        <p align="justify" style="font-size:13px">We explore the procedurally synthetic 3D data generation paradigm to equip individuals with the full capability of creating large-scale annotated photogrammetry point clouds. Specifically, we introduce a synthetic aerial photogrammetry point clouds generation pipeline that takes full advantage of open geospatial data sources and off-the-shelf commercial packages..</p>
                        <p></p>
                    </td>
                </tr>



            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/IASSD.gif" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2203.11139">
                            <papertitle>Not All Points Are Equal: Learning Highly Efficient Point-based Detectors for 3D LiDAR Point Clouds
                            </papertitle>
                        </a>
                            <br>Y. Zhang, <strong>Q. Hu*</strong>, G. Xu, Y. Ma, J. Wan, Y. Guo<br>
                            (*indicates corresponding author)<br>
                            <em>CVPR 2022 </em><font color="red"><strong>(Oral)</strong></font><br>
                            <a href="https://arxiv.org/abs/2203.11139">Paper</a>/
                            <a href="https://www.youtube.com/watch?v=3jP2o9KXunA">Demo</a> /
                            <a href="https://github.com/yifanzhang713/IA-SSD"><font color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=yifanzhang713&repo=IA-SSD&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px"
                                    height="20px"></iframe>

                        <p align="justify" style="font-size:13px">We study the problem of efficient object detection of 3D LiDAR point clouds. In this paper, we propose a highly-efficient single-stage point-based 3D detector in this paper, termed IA-SSD, based on the fact that foreground points are inherently more important than background points for object detectors.</p>
                        <p></p>
                    </td>
                </tr>



            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/Dynamic.gif" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2203.11113">
                            <papertitle>No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static Models by Fitting Feature-level Space-time Surfaces
                            </papertitle>
                        </a>
                            <br>J. Zhong, K. Zhou, <strong>Q. Hu*</strong>, B. Wang, N. Trigoni, A. Markham<br>
                            (*indicates corresponding author)<br>
                            <em>CVPR 2022 </em><br>
                            <a href="https://arxiv.org/abs/2203.11113">Paper</a>/
                            <a href="https://github.com/jx-zhong-for-academic-purpose/Kinet"><font color="red">Code</font></a><iframe src="https://ghbtns.com/github-btn.html?user=jx-zhong-for-academic-purpose&repo=Kinet&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px"
                                    height="20px"></iframe>

                        <p align="justify" style="font-size:13px">Scene flow is a powerful tool for capturing the motion field of 3D point clouds. To capture 3D motions without explicitly tracking correspondences, we propose a kinematics-inspired neural network (Kinet) by generalizing the kinematic concept of ST-surfaces to the feature space.</p>
                        <p></p>
                    </td>
                </tr>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/3DAC.png" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2203.09931">
                            <papertitle>3DAC: Learning Attribute Compression for Point Clouds
                            </papertitle>
                        </a>
                            <br>G. Fang, <strong>Q. Hu</strong>, H. Wang, Y. Xu, Y. Guo<br>
                            <em>CVPR 2022 </em><br>
                            <a href="https://arxiv.org/abs/2203.09931">Paper</a>
                        <p align="justify" style="font-size:13px">We study the problem of attribute compression for large-scale unstructured 3D point clouds. Through an in-depth exploration of the relationships between different encoding steps and different attribute channels, we introduce a deep compression network, termed 3DAC, to explicitly compress the attributes of 3D point clouds and reduce storage usage in this paper.</p>
                        <p></p>
                    </td>
                </tr>



            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/sensaturban.gif" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://link.springer.com/article/10.1007/s11263-021-01554-9">
                            <papertitle>SensatUrban: Learning Semantics from Urban-Scale Photogrammetric Point Clouds
                            </papertitle>
                        </a>
                            <br><strong>Q. Hu</strong>, B. Yang, S. Khalid, W. Xiao, N. Trigoni, A.
                                        Markham<br>
                            <em>International Journal of Computer Vision,
                                2022 </em><strong>(IF=7.41)</strong><br>
                            <a href="https://link.springer.com/article/10.1007/s11263-021-01554-9">Paper</a> /
                            <a href="https://www.youtube.com/watch?v=IG0tTdqB3L8">Demo</a> /
                             <a href="https://github.com/QingyongHu/SensatUrban"><font
                                                color="red">Code</font></a>
                                        <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SensatUrban&type=star&count=true&size=small"
                                                frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                        <p align="justify" style="font-size:13px">This is a journal extension of the SensatUbran dataset, with more extensive experiments on the cross-dataset generalization, semantic learning with fewer labels and self-supervised pre-training. We provide more details regarding the data acquisition and semantic annotation.</p>
                        <p></p>
                    </td>
                </tr>




            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/S3DIS_area2.gif" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://ieeexplore.ieee.org/document/9440696">
                            <papertitle>Learning Semantic Segmentation of Large-Scale Point Clouds with Random Sampling
                            </papertitle>
                        </a>
                            <br><strong>Q. Hu</strong>, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang,
                            N. Trigoni, A. Markham<br>
                            <em>IEEE Transactions on Pattern Analysis and Machine Intelligence,
                                2021 </em><strong>(IF=17.73)</strong><br>
                            <a href="https://ieeexplore.ieee.org/document/9440696">Paper</a> /
                            <a href="https://youtu.be/Ar3eY_lwzMk">Demo</a> /
                            <a href="https://github.com/QingyongHu/RandLA-Net"><font
                                    color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=RandLA-Net&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px"
                                    height="20px"></iframe>
                        <p align="justify" style="font-size:13px">This is a journal extension of the RandLA-Net, with more extensive experiments on the Toronto3D, NPM3D, ScanNet and DALES. We provide more details
                            regarding the entire architecture and the implementation of our RandLA-Net, including all
                            specifics of encoding/decoding neural layers and particulars of the training strategy.</p>
                        <p></p>
                    </td>
                </tr>


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/SQN_2.gif" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2104.04891">
                            <papertitle>SQN: Weakly-Supervised Semantic Segmentation of Large-Scale 3D Point Clouds
                                with 1000× Fewer Labels
                            </papertitle>
                        </a>
                            <br><strong>Q. Hu</strong>, B. Yang, G. Fang, A. Leornadis, Y. Guo, N. Trigoni, A.
                            Markham<br>
                            <em> Arxiv, 2021</em>
                            <br>
                            <!--<font color="red"><strong>..</strong></font><br>-->
                            <a href="https://arxiv.org/abs/2104.04891">arXiv</a> / 
                            <a href="https://www.youtube.com/watch?v=Q6wICSRRw3s&t=4s">Demo</a> /
                            <a href="https://www.youtube.com/watch?v=N0UAeY31msY&t=4s">Annotation</a> /
                            <a href="https://github.com/QingyongHu/SQN"><font color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SQN&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                        <p align="justify" style="font-size:13px"> We propose a new weak supervision method to
                            implicitly augment the total amount of available supervision signals, by leveraging the
                            semantic similarity between neighboring points. Extensive experiments demonstrate that
                            the proposed Semantic Query Net- work (SQN) achieves state-of-the-art performance on six large-scale open datasets under weak supervision schemes, while requiring only 1‰ labeled points for training.</p>
                        <p></p>
                    </td>
                </tr>


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/SpinNet_2020.png" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2011.12149">
                            <papertitle>SpinNet: Learning a General Surface Descriptor for 3D Point Cloud
                                Registration
                            </papertitle>
                        </a>
                            <br>S. Ao*, <strong>Q. Hu*</strong>, B. Yang, A. Markham, Y. Guo<br>
                            <em> CVPR, 2021</em>
                            (* indicates equal contribution)
                            <br>
                            <!--<font color="red"><strong>..</strong></font><br>-->
                            <a href="https://arxiv.org/abs/2011.12149">arXiv</a> /
                            Demo /
                            Project Page /
                            <a href="https://github.com/QingyongHu/SpinNet"><font color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=The-Learning-And-Vision-Atelier-LAVA&repo=SpinNet&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                        <p align="justify" style="font-size:13px">
                         We introduce a new, yet conceptually simple,neural architecture to extract local features which are rotationally invariant whilst sufficiently informative to enable accurate registration. A Spatial Point Transformer is first introduced to map the input local surface into a carefully designed cylindrical space, enabling end-to-end optimization with SO(2) equivariant representation. A  Neural  Feature Extractor which leverages the powerful point-based and 3D cylindrical convolutional  neural layers is then utilized to derive a compact and representative descriptor for matching.
                        </p>
                        <p></p>
                    </td>
                </tr>


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/3DV2020.gif" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="http://arxiv.org/abs/2009.03137">
                            <papertitle>Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A
                                Dataset, Benchmarks and Challenges
                            </papertitle>
                        </a>
                            <br><strong>Q. Hu</strong>, B. Yang, S. Khalid, W. Xiao, N. Trigoni, A.
                            Markham<br>
                            <em> CVPR, 2021</em>
                            <br>
                            <!--<font color="red"><strong>..</strong></font><br>-->
                            <a href="http://arxiv.org/abs/2009.03137">arXiv</a> /
                            <a href="https://www.youtube.com/watch?v=IG0tTdqB3L8">Demo</a> /
                            <a href="http://point-cloud-analysis.cs.ox.ac.uk/">Project Page</a> /
                            <a href="https://github.com/QingyongHu/SensatUrban"><font
                                    color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SensatUrban&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                        <p align="justify" style="font-size:13px">We introduce an urban-scale
                            photogrammetric point cloud dataset with nearly three billion richly annotated points, which is five times the number of labeled points than the existing largest point cloud dataset. We extensively evaluate the performance of state-of-the-art algorithms on our dataset and provide a comprehensive analysis of the results. In particular, we identify several key challenges towards urban-scale point cloud understanding.</p>
                        <p></p>
                    </td>
                </tr>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/VISO.gif" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2111.12960">
                            <papertitle>Detecting and Tracking Small and Dense Moving Objects in Satellite Videos: A Benchmark
                            </papertitle>
                        </a>
                            <br>Q. Yin, <strong>Q. Hu*</strong>, H. Liu, F. Zhang, Y. Wang, Z. Lin, W. An, Y. Guo<br>
                            (*indicates equal contribution)<br>
                            <em>IEEE Transactions on Geoscience and Remote Sensing </em><strong>(IF=5.6)</strong><br>
                            <a href="https://arxiv.org/abs/2111.12960">Paper</a>/
                            <a href="https://www.youtube.com/shorts/NctUdpQBbAU">Demo</a> /
                            <a href="https://github.com/The-Learning-And-Vision-Atelier-LAVA/VISO">Project Page</a> /
                            <a href="https://github.com/The-Learning-And-Vision-Atelier-LAVA/VISO"><font
                                    color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=The-Learning-And-Vision-Atelier-LAVA&repo=VISO&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px"
                                    height="20px"></iframe>
                        <p align="justify" style="font-size:13px">In this paper, we first build a large-scale satellite video dataset with rich annotations for the task of moving object detection and tracking. This dataset is collected by the Jilin-1 satellite constellation and composed of 47 high-quality videos with 1,646,038 instances of interest for object detection and 3,711 trajectories for object tracking. We</p>
                        <p></p>
                    </td>
                </tr>


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/TPAMI_20.png" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/1912.12033">
                            <papertitle>Deep Learning for 3D Point Clouds: A Survey</papertitle>
                        </a>
                            <br>Y. Guo*, H. Wang*, <strong>Q. Hu*</strong>, H. Liu*, L. Liu, M.
                            Bennamoun<br>
                            <em>IEEE Transactions on Pattern Analysis and Machine Intelligence,
                                2020 </em><strong>(IF=17.73)</strong><br>
                            (* indicates equal contribution)
                            <br>
                            <!--<font color="red"><strong>..</strong></font><br>-->
                            <a href="https://arxiv.org/abs/1912.12033">arXiv</a> /
                            <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:6VVjXDV6gtUJ:scholar.google.com/&output=citation&scisdr=CgXbiqGAEK2E47GwA08:AAGBfm0AAAAAXkG1G0-_6jiOUBjsmdbInX93RmVbuF2Q&scisig=AAGBfm0AAAAAXkG1G8OBu2xYQAAGnnZ4qY9756tBB5rp&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>
                            /
                            <font color="blue"> News:</font>
                            <a href="https://mp.weixin.qq.com/s/5RJAv_cOlhee1R9uZzkmHQ"><font
                                    color="blue">(专知,</font></a>
                            <a href="https://mp.weixin.qq.com/s/AgbDICM8hDJIfKjLm_hCIQ"><font
                                    color="blue">CVer) / </font></a>
                            <a href="https://github.com/QingyongHu/SoTA-Point-Cloud"><font color="red">Project
                                page</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=The-Learning-And-Vision-Atelier-LAVA&repo=SoTA-Point-Cloud&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px" height="20px"></iframe>

                        <p align="justify" style="font-size:13px"> We presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classification, 3D object detection, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.</p>
                        <p></p>
                    </td>
                </tr>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/CVPR20_SemanticKITTI.gif" alt="PontTuset"
                                         width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/1911.11236">
                            <papertitle>RandLA-Net: Efficient Semantic Segmentation of Large-Scale
                                Point
                                Clouds
                            </papertitle>
                        </a>
                            <br><strong>Q. Hu</strong>, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang,
                            N. Trigoni, A. Markham<br>
                            <em> Computer Vision and Pattern Recoginition (CVPR), 2020</em> <font
                                    color="red"><strong>(Oral, 335/6656)</strong></font><br>
                            <!--<font color="red"><strong>..</strong></font><br>-->
                            <a href="https://arxiv.org/abs/1911.11236">arXiv</a> /
                            <a href="https://youtu.be/Ar3eY_lwzMk">Demo</a> /
                            <font color="blue"> News:</font>
                            <a href="https://mp.weixin.qq.com/s/k_oROm1Zr6l0YNKGELx3Bw"><font
                                    color="blue">(新智元,</font></a>
                            <a href="https://mp.weixin.qq.com/s/xuLJ8m_ipGVBXVduA7Y0IA"><font
                                    color="blue">极市平台,</font></a>
                            <a href="https://mp.weixin.qq.com/s/uTMTDykmmF9CahSwotlksw"><font
                                    color="blue">AI科技评论)
                                / </font></a>
                            <a href="https://github.com/QingyongHu/RandLA-Net"><font
                                    color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=RandLA-Net&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px"
                                    height="20px"></iframe>
                        <p align="justify" style="font-size:13px">We propose a simple and efficient
                            neural architecture for 3D semantic segmentation on large-scale point
                            clouds. It achieves the SOTA performance on Semantic3D and SemanticKITTI
                            (Nov 2019), with up to 200x fast than existing approaches.</p>
                        <p></p>
                    </td>
                </tr>


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/19_NeurIPS_scannet_scene0015.gif"
                                         alt="PontTuset"
                                         width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/1906.01140">
                            <papertitle>Learning Object Bounding Boxes for 3D Instance
                                Segmentation
                                on Point Clouds
                            </papertitle>
                        </a>
                            <br>B. Yang, J. Wang, R. Clark, <strong>Q. Hu</strong>, S. Wang, A.
                            Markham,
                            N.
                            Trigoni<br>
                            <em>Neural Information Processing Systems (NeurIPS)</em>, 2019 <font
                                    color="red"><strong>(Spotlight, 200/6743)</strong></font>
                            <br>
                            <!--<font color="red"><strong>..</strong></font><br>-->
                            <a href="https://arxiv.org/abs/1906.01140">arXiv</a>/
                            <a href="https://www.reddit.com/r/MachineLearning/comments/bx8jhz/r_new_sota_for_3d_object_detection/">Reddit</a>/
                            <a href="https://www.youtube.com/watch?v=Bk727Ec10Ao">Demo</a>/
                            <font color="blue"> News:</font>
                            <a href="https://mp.weixin.qq.com/s/jHbWf_SSZE_J6NRJR-96sQ"><font
                                    color="blue">(新智元,</font></a>
                            <a href="https://mp.weixin.qq.com/s/wViZITtsb4j3oFtOpJI9wQ"><font
                                    color="blue">将门创投,</font></a>
                            <a href="https://mp.weixin.qq.com/s/gybhVw3D4ykAHsVGzazWNw"><font
                                    color="blue">泡泡机器人)</font>/</a>
                            <a href="https://github.com/Yang7879/3D-BoNet"><font
                                    color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=Yang7879&repo=3D-BoNet&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px"
                                    height="20px"></iframe>

                        <p align="justify" style="font-size:13px">We propose a simple and efficient neural architecture for accurate 3D instance segmentation on point clouds. It achieves the SOTA performance on ScanNet and S3DIS (June 2019).</p>
                        <p></p>
                    </td>
                </tr>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/BMVC2020.png" alt="PontTuset"
                                         width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2008.02312">
                            <papertitle>Axiom-based Grad-CAM: Towards Accurate Visualization
                                and
                                Explanation
                                of CNNs
                            </papertitle>
                        </a>
                            <br>R. Fu, <strong>Q. Hu</strong>, X. Dong, Y. Guo, Y. Gao, B.
                            Li<br>
                            <em>British Machine Vision Conference (BMVC)</em>, 2020 <font
                                    color="red"><strong>(Oral, 30/670)</strong></font> <br>
                            <!--<font color="red"><strong>..</strong></font><br>-->
                            <a href="https://arxiv.org/abs/2008.02312">arXiv</a>/
                            <a href="https://github.com/Fu0511/XGrad-CAM"><font
                                    color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=Fu0511&repo=XGrad-CAM&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px"
                                    height="20px"></iframe>

                        <p align="justify" style="font-size:13px"> We introduce two axioms
                            --Conservation and Sensitivity -- to the visualization paradigm of the CAM methods. Meanwhile, a dedicated Axiom-based Grad-CAM (XGrad-CAM) is proposed to satisfy these axioms as much as possible. Experiments demonstrate that XGrad-CAM is able to achieve better visualization performance than Grad-CAM, while also be class-discriminative and easy-to-implement compared with Grad-CAM++ and Ablation-CAM.</p>
                        <p></p>
                    </td>
                </tr>


            <table width="100%" align="center" border="0" cellspacing="0"
                   cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/cGAN-Net.png" alt="PontTuset"
                                         width="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2008.02312">
                            <papertitle>CGAN-Net: Class-Guided Asymmetric Non-local
                                Network
                                for
                                Real-time Semantic Segmentation
                            </papertitle>
                        </a>
                            <br>H. Chen, <strong>Q. Hu</strong>, J. Yang, J. Wu, Y.
                            Guo<br>
                            <em>International Conference on Acoustics, Speech, & Signal
                                Processing (ICASSP)</em>, 2021 <br>
                            <!--<font color="red"><strong>..</strong></font><br>-->

                        <p align="justify" style="font-size:13px"> we introduce a
                            Class-Guided Asymmetric Non-local Network (CGAN-Net) to
                            enhance the class-discriminability in learned feature map,
                            while
                            maintaining real-time efficiency. The key to our approach is
                            to
                            calculate the dense similarity matrix in coarse semantic
                            prediction
                            maps, instead of the high-dimensional latent feature
                            map.</p>
                        <p></p>
                    </td>
                </tr>


                    <td width="20%"><img src="./imgs/TIM_LH.gif" alt="PontTuset" width="180"
                                         style="border-style: none">
                    </td>
                    <td width="80%" valign="top">
                        <p><a href="https://ieeexplore.ieee.org/document/8708248">
                            <papertitle>Robust Long-term Tracking via Instance Specific
                                Proposals
                            </papertitle>
                        </a>
                            <br>H. Liu, <strong>Q. Hu</strong>, B. Li, Y. Guo<br>
                            <em>IEEE Transactions on Instrumentation and Measurement (IEEE
                                TIM)</em>,
                            2019
                            <strong>(IF=3.06)</strong><br>
                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We propose an efficient and robust tracker for long-term object tracking, which is based on instance specific proposals. In particular, an instance-specific proposal generator is embedded into the error correction module to recover lost target from tracking failures. </p>
                    </td>
                    </tr>


                    <td width="20%"><img src="./imgs/mot.gif" alt="PontTuset" width="180"
                                         style="border-style: none">
                    </td>
                    <td width="80%" valign="top">
                        <p><a href="https://ieeexplore.ieee.org/document/8474304">
                            <papertitle>Semi-Online Multiple Object Tracking Using Graphical
                                Tracklet
                                Association
                            </papertitle>
                        </a>
                            <br>J. Wang, Y. Guo, X. Tang, <strong>Q. Hu</strong>, W. An<br>
                            <em>IEEE Signal Processing Letters (IEEE SPL)</em>, 2018
                            <strong>(IF=3.27)</strong><br>
                            <a href="https://www.youtube.com/watch?v=Wfsu8NVoPJs">Demo</a>
                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px"> We propose a semi-online
                            MOT method using online discriminative appearance learning and tracklet association with a sliding window. We connect similar detections of neighboring frames in a temporal window, and improve the performance of appearance feature by online discriminative appearance learning. Then, tracklet association is performed by minimizing a subgraph decomposition cost.</p>
                    </td>
                    </tr>

                    <td width="20%"><img src="./imgs/SOT.gif" alt="PontTuset"
                                         width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://ieeexplore.ieee.org/document/8012414">
                            <papertitle>Object tracking using multiple features and adaptive
                                model
                                updating
                            </papertitle>
                        </a>
                            <br><strong>Q. Hu</strong>, Y. Guo, Z. Lin, W. An, H. Cheng<br>
                            <em>IEEE Transactions on Instrumentation and Measurement (IEEE
                                TIM)</em>,
                            2018
                            <strong>(IF=3.06)</strong><br>
                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We proposea to combine a
                            2-D location filter with a 1-D scale filter to jointly estimate the state of object under tracking, and three complementary features are integrated to further enhance the overall tracking performance. A penalty factor is also defined to achieve a balance between stability and flexibility, especially when the object is under occlusion.
                        </p>
                    </td>
                    </tr>


                    <td width="20%"><img src="./imgs/ICPR.gif" alt="PontTuset"
                                         width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://ieeexplore.ieee.org/document/8545780">
                            <papertitle>Long-term Object Tracking with Instance Specific
                                Proposals
                            </papertitle>
                        </a>
                            <br>H. Liu, <strong>Q. Hu</strong>, B. Li, Y. Guo<br>
                            <em>24th International Conference on Pattern Recognition
                                (ICPR)</em>,
                            2018<br>
                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We propose a tracker named Complementary Learners with Instance-specific Proposals (CLIP). The CLIP tracker consists of a translation filter, a scale filter, and an error correction module. The error correction module is activated to correct the localization error by an instance-specific proposal generator, especially when the target suffers from dramatic appearance changes.</p>
                    </td>
                    </tr>


                    <td width="20%"><img src="./imgs/BMVC.gif" alt="PontTuset"
                                         width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/1802.00411">
                            <papertitle>Correlation Filter Tracking: Beyond an Open-loop
                                System
                            </papertitle>
                        </a>
                            <br><strong>Q. Hu</strong>, Y. Guo, Y. Chen, J. Xiao, W. An<br>
                            <em>British Machine Vision Conference (BMVC)</em>, 2017 <br>
                            <a href="https://www.youtube.com/watch?v=dN9L1O_9ZB0">Demo</a> /
                            <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:ZKBm6H6GuZ0J:scholar.google.com/&output=citation&scisdr=CgXbiqGAEPeF4LGz-rw:AAGBfm0AAAAAXkG24rx8mDZeQvERuq5dFujB1zRmoq3r&scisig=AAGBfm0AAAAAXkG24jggLsN29RWI7IaH4jKtwLHgmh8i&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>
                            /
                            <a href="https://github.com/QingyongHu/Correlation-Filter-Tracking-Beyond-an-Open-loop-System"><font
                                    color="red">Code</font> </a>
                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We interpret object
                            tracking as a closed-loop tracking problem, and add a feedback loop to the tracking process by introducing an efficient method to estimate the localization error. We propose a generic self-correction mechanism for CF based trackers by introducing a closed-loop feedback technique.</p>
                    </td>
                    </tr>


                    <td width="20%"><img src="./imgs/DICTA.gif" alt="PontTuset"
                                         width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://ieeexplore.ieee.org/document/7797075">
                            <papertitle>Robust and real-time object tracking using
                                scale-adaptive
                                correlation
                                filters
                            </papertitle>
                        </a>
                            <br><strong>Q. Hu</strong>, Y. Guo, Z. Lin, X. Deng, W. An<br>
                            <em>Digital Image Computing: Techniques and Applications
                                (DICTA)</em>, 2016 </em><font
                                    color="black"><strong>(Oral)</strong></font><br>
                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We represent the target in kernel feature space and train a classifier on a scale pyramid to achieve adaptive scale estimation. We then integrate three complementary features to further enhance the overall tracking performance.</p>
                    </td>
                    </tr>

                    </tbody>
                </table>


                <!--SECTION 6 -->
                <table width="100%" align="center" border="0" cellspacing="0"
                       cellpadding="20">
                    <tbody>
                    <tr>
                        <td>
                            <heading>Teaching</heading>
                            <p><strong>Trinity Term, 2019</strong>: &ensp;&ensp; <a
                                    href="https://www.cs.ox.ac.uk/teaching/courses/2019-2020/ai/">
                                Artificial Intelligence
                            </a> (University of Oxford). </p>

                            <p><strong>Hilary Term, 2020</strong>: &ensp;&ensp; <a
                                    href="https://www.cs.ox.ac.uk/teaching/courses/2019-2020/ai/">
                                Artificial Intelligence
                            </a> (University of Oxford). </p>

                            <p><strong>Trinity Term, 2020</strong>: &ensp;&ensp; <a
                                    href="https://www.cs.ox.ac.uk/teaching/courses/2019-2020/ml/">
                                Machine Learning
                            </a> (University of Oxford). </p>

                            <p><strong>Trinity Term, 2020</strong>: &ensp;&ensp; <a
                                    href="https://www.cs.ox.ac.uk/teaching/courses/2019-2020/graphics/">
                                Computer Graphics
                            </a> (University of Oxford). </p>

                        </td>
                    </tr>
                    </tbody>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0"
                       cellpadding="20">
                    <tr>
                        <td width="100%" valign="middle">
                            <heading>Reviewer Services</heading>
                            <ul style="list-style-type:disc;">
                                <li>
                                    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">
                                        IEEE Transactions on Pattern Analysis and Machine
                                        Intelligence (IEEE TPAMI)
                                <li>
                                    <a href="https://www.springer.com/journal/11263">
                                        International Journal of Computer Vision (IJCV)
                                <li>
                                    <a href="https://iclr.cc/">
                                        International Conference on Learning Representations
                                        (ICLR)
                                <li>
                                    <a href="https://icml.cc/">
                                        International Conference on Machine Learning
                                        (ICML)
                                <li>
                                    <a href="https://nips.cc/">
                                        Conference on Neural Information Processing Systems
                                        (NeurIPS)
                                <li>
                                    <a href="http://cvpr2021.thecvf.com/">
                                        IEEE Conference on Computer Vision and Pattern
                                        Recognition (CVPR)
                                <li>
                                    <a href="http://iccv2021.thecvf.com/home">
                                        International Conference on Computer Vision (ICCV)
                                <li>
                                    <a href="http://iccv2021.thecvf.com/home">
                                        European Conference on Computer Vision (ECCV)
                                <li>
                                    <a href="http://www.icra2021.org/">
                                        International Conference on Robotics and
                                        Automation (ICRA)
                                <li>
                                    <a href="http://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros">
                                        IEEE/RSJ International Conference on Intelligent
                                        Robots
                                        and Systems (IROS)
                                <li><a href="https://bmvc2020.github.io/"> British
                                    Machine Vision Conference (BMVC)
                                <li>
                                    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">
                                        IEEE Transactions on Image Processing (IEEE TIP)
                                <li>
                                    <a href="https://www.sciencedirect.com/journal/information-fusion">
                                        IEEE Transactions on Circuits and Systems for Video
                                        Technology (IEEE TCSVT)
                                <li>
                                    <a href="https://www.journals.elsevier.com/computers-and-graphics">
                                        Computers & Graphics (C&G)
                                <li>
                                    <a href="https://www.sciencedirect.com/journal/information-fusion">
                                        Information Fusion
                            </ul>
                        </td>
                    </tr>
                </table>


                <!--SECTION 9 -->
                <div style="clear:both;">
                    <p align="right"><font size="2">
                        <script type="text/javascript" id="clustrmaps"
                                src="//cdn.clustrmaps.com/map_v2.js?d=liidPLPwwYqJmgAk1lXn75bsZTzJiDg-yWRt9b6CKwc&cl=ffffff&w=a"></script>
                    </font></p>
                    <br/>
                </div>


                <!--SECTION 10 -->
                <table width="100%" align="center" border="0" cellspacing="0"
                       cellpadding="20">
                    <tbody>
                    <tr>
                        <td><br>
                            <!--<p align="right"><font size="3">Erd&ouml;s = ? </font><br> -->
                            <p align="right"><font size="2"> Last update: 2020.02.10. <a
                                    href="http://www.cs.berkeley.edu/~barron/">Thanks.</a></font>
                            </p>
                        </td>
                    </tr>
                    </tbody>
                </table>


                </td>
                </tr>
                </tbody>
            </table>
</body>
</html>
