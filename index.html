<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus, a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body, td, th, tr, p, a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        .hp-photo {
            width: 240px;
            height: 240px;
            border-radius: 240px;
            -webkit-border-radius: 240px;
            -moz-border-radius: 240px;
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 24px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }
    </style>

    <title>Qingyong Hu | University of Oxford</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="./imgs/oxford_icon.jpg">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody>
    <tr>
        <td>


            <!--SECTION 1 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="68%" valign="middle">
                        <p align="center">
                            <name>Qingyong Hu</name>
                        </p>
                        <p align="justify">I am a D.Phil student (Oct 2018 - ) in the <a href="http://www.cs.ox.ac.uk/">
                            Department of Computer Science</a> at the <a href="http://www.ox.ac.uk/">University of
                            Oxford</a>, supervised by <a href="https://www.cs.ox.ac.uk/people/niki.trigoni/"> Profs.
                            Niki Trigoni</a> and <a href="https://www.cs.ox.ac.uk/people/andrew.markham/">Andrew
                            Markham</a>. Prior to Oxford, I obtained my M.Eng degree and B.Eng degree from China and
                            supervised by <a href="http://yulanguo.me/">Profs. Yulan Guo</a>.

                            <!--                    </br></br>-->
                            <!--                    In this summer (July - Oct 2019), I was a research intern at the Augumented Reality team of <a href="http://www.a9.com/">Amazon</a> (Palo Alto, CA).-->
                            <!--                    In my M.Phil study, I interned at <a href="https://www.astri.org/">Hong Kong Applied Science and Technology Research Institute</a>.-->
                            <!--                    In my undergraduate study, I was an exchange student at <a href="http://www.upv.es/">Universitat Politècnica de València</a> (Valencia, Spain).-->

                            </br>
                        </p>
                        <p align="center">
                            <a href="mailto:qingyong.hu@cs.ox.ac.uk">Email</a> /
                            <a href="https://github.com/QingyongHu"> Github </a> /
                            <a href="https://www.zhihu.com/people/hu-qing-yong"> Blog </a> /
                            <a href="https://www.linkedin.com/in/qingyong-hu-b18061171/"> LinkedIn </a> /
                            <a href="https://twitter.com/home"> Twitter </a> /
                            <a href="https://scholar.google.com/citations?user=yboFNHEAAAAJ&hl=en">Google Scholar</a>


                        </p>
                    </td>
                    <td align="right"><img class="hp-photo" src="./imgs/photo.jpg" style="width: 240;"></td>
                </tr>
                </tbody>
            </table>

            <!--SECTION 2 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Research</heading>
                        <p align="justify">
                            I am interested in 3D computer vision, machine learning, and robotics. My research goal is
                            to build intelligent systems which are able to achieve an effective and efficient perception
                            and understanding of 3D scenes. In particular, my research focuses on large-scale point
                            cloud segmentation, dynamic point cloud processing, point cloud tracking, and local surface
                            matching. If you are interested in my research or have any use cases that you want to share,
                            feel free to <a href="mailto:qingyong.hu@cs.ox.ac.uk">contact me</a>!
                            <!--</br></br>-->
                            <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!--SECTION 3 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>News</heading>
                        <p><strong>[2021.03.01]</strong> Two papers (<a href="http://arxiv.org/abs/2009.03137">SensatUrban</a>,
                            <a href="https://arxiv.org/abs/2011.12149">SpinNet</a>) are accepted to CVPR 2021!
                        <p><strong>[2021.01.30]</strong> One paper on efficient semantic segmentation is accepted to
                            ICASSP 2021!
                        <p><strong>[2020.11.25]</strong> Our <a href="https://arxiv.org/abs/2011.12149">SpinNet</a>
                            (3D point cloud registration) is on arXiv!
                        <p><strong>[2020.10.15]</strong> our <a href="https://github.com/QingyongHu/RandLA-Net">RandLA-Net</a>
                            has been integrated to <a
                                    href="http://www.open3d.org/docs/release/python_api/open3d.ml.tf.models.RandLANet.html">Open3d</a>
                            as a standard module!
                        <p><strong>[2020.10.09]</strong> Invited to present our work on
                            <a href="https://wonderlandai.com/"> Wonderland AI Summit 2020</a>!
                        <p><strong>[2020.09.22]</strong> Invited to present our recent works on
                            <a href="https://sites.google.com/view/3d-dlad-v2-iv2020/schedule/"> 2nd Workshop on
                                3D-DLAD </a> at <a href="https://2020.ieee-iv.org/"> IEEE Intelligent Vehicles 2020</a>!
                        <p><strong>[2020.09.22]</strong> We will organize a tutorial of
                            <a href="http://3dlearning.cn/"> 3D Point Cloud Reconstruction and Segmentation </a>
                            in the <a href="http://3dv2020.dgcv.nii.ac.jp/"> 3DV 2020</a>!
                        <p><strong>[2020.09.08]</strong> Our <a href="http://arxiv.org/abs/2009.03137">SensatUrban</a>
                            (Urban-scale photogrammetric point cloud) dataset is on arXiv!</p>
                        <p><strong>[2020.08.05]</strong> One paper on <a href="https://arxiv.org/abs/2008.02312"> CNN
                            intepretation</a> is accepted as oral presentation at<a href="https://bmvc2020.github.io/">
                            BMVC2020</a>!</p>
                        <p><strong>[2020.06.23]</strong> Our <a href="https://arxiv.org/abs/1912.12033">point cloud
                            survey</a> paper has been accepted to IEEE TPAMI! </a></p>
                        <p><strong>[2020.03.08]</strong> Invited to present our <a
                                href="https://arxiv.org/abs/1911.11236">RandLA-Net</a>
                            and <a href="https://arxiv.org/abs/1906.01140">3D-BoNet</a> at Shenlan.
                            Here are the <a href="https://www.shenlanxueyuan.com/open/course/53">Video</a>
                            and
                            <a href="https://www.dropbox.com/s/80w91bqzfdl5vow/%E6%B7%B1%E8%93%9D%E5%AD%A6%E9%99%A2%E5%85%AC%E5%BC%80%E8%AF%BE_20200308.pdf?dl=0">Slides</a>.
                        </p>
                        <p><strong>[2020.03.02]</strong> The <a href="https://github.com/QingyongHu/RandLA-Net">code</a>
                            for our <a href="https://arxiv.org/abs/1911.11236">RandLA-Net</a> is available now! </a></p>
                        <p><strong>[2020.02.24]</strong> Our <a href="https://arxiv.org/abs/1911.11236">RandLA-Net</a>
                            is accepted by CVPR2020! </a></p>
                        <p><strong>[2020.01.13]</strong> Successfully defend <a
                                href="https://www.ox.ac.uk/students/academic/guidance/graduate/research/status/DPhil?wssl=1">D.Phil
                            transfer</a> viva, examined by <a
                                href="http://www.cs.ox.ac.uk/people/alex.rogers/"> Profs. Alex Rogers</a> and <a
                                href="http://www.robots.ox.ac.uk/~victor/">Profs. Victor Adrian Prisacariu</a>.</p>
                        <p><strong>[2019.12.27]</strong> One co-first authored survey paper on point clouds is on <a
                                href="https://arxiv.org/abs/1912.12033">arXiv</a>!</p>
                        <p><strong>[2019.12.13]</strong> Attending and presenting our RandLA-Net on the <a
                                href="https://www.turing.ac.uk/collaborate-turing/data-study-groups">Turing Data Study
                            Group</a>!</p>
                        <p><strong>[2019.11.25]</strong> Our <a href="https://arxiv.org/abs/1911.11236">RandLA-Net</a>
                            is on arXiv!</p>
                        <p><strong>[2019.09.03]</strong> One paper on 3D instance segmentation is accepted as a
                            spotlight at <a href="https://nips.cc/Conferences/2019">NeurIPS 2019</a>. </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!--SECTION 4 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Publications / Preprints</heading>
                    </td>
                </tr>
                </tbody>
            </table>


            <!--SECTION 5 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="20%"><img src="./imgs/SpinNet_2020.png" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2011.12149">
                            <papertitle>SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration
                            </papertitle>
                        </a>
                            <br>S. Ao*, <strong>Q. Hu*</strong>, B. Yang, A. Markham, Y. Guo<br>
                            <em> CVPR, 2021</em>
                            (* indicates equal contribution)
                            <br>
                            <!--<font color="red"><strong>..</strong></font><br>-->
                            <a href="https://arxiv.org/abs/2011.12149">arXiv</a> /
                            Demo /
                            Project Page /
                            <a href="https://github.com/QingyongHu/SpinNet"><font color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SpinNet&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                        <p align="justify" style="font-size:13px">We introduce a new, yet conceptually simple, neural
                            architecture to extract local features which are rotationally invariant
                            whilst sufficiently informative to enable accurate registration. A Spatial Point Transformer
                            is first introduced to map the input local surface into a carefully designed cylindrical
                            space, enabling end-to-end optimization with SO(2) equivariant representation. A Neural
                            Feature Extractor which leverages the powerful point-based and 3D cylindrical convolutional
                            neural layers is then utilized to derive a compact and representative descriptor for
                            matching.</p>
                        <p></p>
                    </td>
                </tr>


                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                    <tr>
                        <td width="20%"><img src="./imgs/3DV2020.gif" alt="PontTuset" width="180"
                                             style="border-style: none"></td>
                        <td width="80%" valign="top">
                            <p><a href="http://arxiv.org/abs/2009.03137">
                                <papertitle>Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset,
                                    Benchmarks and Challenges
                                </papertitle>
                            </a>
                                <br><strong>Q. Hu</strong>, B. Yang, S. Khalid, W. Xiao, N. Trigoni, A. Markham<br>
                                <em> CVPR, 2021</em>
                                <br>
                                <!--<font color="red"><strong>..</strong></font><br>-->
                                <a href="http://arxiv.org/abs/2009.03137">arXiv</a> /
                                <a href="https://www.youtube.com/watch?v=IG0tTdqB3L8">Demo</a> /
                                <a href="http://point-cloud-analysis.cs.ox.ac.uk/">Project Page</a> /
                                <a href="https://github.com/QingyongHu/SensatUrban"><font color="red">Code</font></a>
                                <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SensatUrban&type=star&count=true&size=small"
                                        frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                            <p align="justify" style="font-size:13px">We introduce an urban-scale photogrammetric point
                                cloud dataset with nearly three billion richly annotated points, which is five times the
                                number of labeled points than the existing largest point cloud dataset. We extensively
                                evaluate the performance of state-of-the-art algorithms on our dataset and provide a
                                comprehensive analysis of the results. In particular, we identify several key challenges
                                towards urban-scale point cloud understanding.</p>
                            <p></p>
                        </td>
                    </tr>


                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                        <tr>
                            <td width="20%"><img src="./imgs/TPAMI_20.png" alt="PontTuset" width="180"
                                                 style="border-style: none"></td>
                            <td width="80%" valign="top">
                                <p><a href="https://arxiv.org/abs/1912.12033">
                                    <papertitle>Deep Learning for 3D Point Clouds: A Survey</papertitle>
                                </a>
                                    <br>Y. Guo*, H. Wang*, <strong>Q. Hu*</strong>, H. Liu*, L. Liu, M. Bennamoun<br>
                                    <em>IEEE Transactions on Pattern Analysis and Machine Intelligence,
                                        2020 </em><strong>(IF=17.73)</strong><br>
                                    (* indicates equal contribution)
                                    <br>
                                    <!--<font color="red"><strong>..</strong></font><br>-->
                                    <a href="https://arxiv.org/abs/1912.12033">arXiv</a> /
                                    <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:6VVjXDV6gtUJ:scholar.google.com/&output=citation&scisdr=CgXbiqGAEK2E47GwA08:AAGBfm0AAAAAXkG1G0-_6jiOUBjsmdbInX93RmVbuF2Q&scisig=AAGBfm0AAAAAXkG1G8OBu2xYQAAGnnZ4qY9756tBB5rp&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>
                                    /
                                    <font color="blue"> News:</font>
                                    <a href="https://mp.weixin.qq.com/s/5RJAv_cOlhee1R9uZzkmHQ"><font
                                            color="blue">(专知,</font></a>
                                    <a href="https://mp.weixin.qq.com/s/AgbDICM8hDJIfKjLm_hCIQ"><font
                                            color="blue">CVer) / </font></a>
                                    <a href="https://github.com/QingyongHu/SoTA-Point-Cloud"><font color="red">Project
                                        page</font></a>
                                    <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SoTA-Point-Cloud&type=star&count=true&size=small"
                                            frameborder="0" scrolling="0" width="120px" height="20px"></iframe>

                                <p align="justify" style="font-size:13px"> We presents a comprehensive review of recent
                                    progress
                                    in deep learning methods for point clouds. It covers three major tasks, including
                                    3D shape classification, 3D object detection, and 3D point cloud segmentation.
                                    It also presents comparative results on several publicly available datasets,
                                    together
                                    with
                                    insightful observations and inspiring future research directions.</p>
                                <p></p>
                            </td>
                        </tr>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                            <tbody>
                            <tr>
                                <td width="20%"><img src="./imgs/CVPR20_SemanticKITTI.gif" alt="PontTuset" width="180"
                                                     style="border-style: none"></td>
                                <td width="80%" valign="top">
                                    <p><a href="https://arxiv.org/abs/1911.11236">
                                        <papertitle>RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point
                                            Clouds
                                        </papertitle>
                                    </a>
                                        <br><strong>Q. Hu</strong>, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N.
                                        Trigoni,
                                        A.
                                        Markham<br>
                                        <em> Computer Vision and Pattern Recoginition (CVPR), 2020</em> <font
                                                color="red"><strong>(Oral, 335/6656)</strong></font><br>
                                        <!--<font color="red"><strong>..</strong></font><br>-->
                                        <a href="https://arxiv.org/abs/1911.11236">arXiv</a> /
                                        <a href="https://youtu.be/Ar3eY_lwzMk">Demo</a> /
                                        <font color="blue"> News:</font>
                                        <a href="https://mp.weixin.qq.com/s/k_oROm1Zr6l0YNKGELx3Bw"><font
                                                color="blue">(新智元,</font></a>
                                        <a href="https://mp.weixin.qq.com/s/xuLJ8m_ipGVBXVduA7Y0IA"><font
                                                color="blue">极市平台,</font></a>
                                        <a href="https://mp.weixin.qq.com/s/uTMTDykmmF9CahSwotlksw"><font color="blue">AI科技评论)
                                            / </font></a>
                                        <a href="https://github.com/QingyongHu/RandLA-Net"><font color="red">Code</font></a>
                                        <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=RandLA-Net&type=star&count=true&size=small"
                                                frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                                    <p align="justify" style="font-size:13px">We propose a simple and efficient neural
                                        architecture
                                        for 3D semantic segmentation on large-scale point clouds.
                                        It achieves the SOTA performance on Semantic3D and SemanticKITTI (Nov 2019),
                                        with up
                                        to
                                        200x
                                        fast than existing approaches.</p>
                                    <p></p>
                                </td>
                            </tr>


                            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                                <tbody>
                                <tr>
                                    <td width="20%"><img src="./imgs/19_NeurIPS_scannet_scene0015.gif" alt="PontTuset"
                                                         width="180"
                                                         style="border-style: none"></td>
                                    <td width="80%" valign="top">
                                        <p><a href="https://arxiv.org/abs/1906.01140">
                                            <papertitle>Learning Object Bounding Boxes for 3D Instance Segmentation on
                                                Point
                                                Clouds
                                            </papertitle>
                                        </a>
                                            <br>B. Yang, J. Wang, R. Clark, <strong>Q. Hu</strong>, S. Wang, A. Markham,
                                            N.
                                            Trigoni<br>
                                            <em>Neural Information Processing Systems (NeurIPS)</em>, 2019 <font
                                                    color="red"><strong>(Spotlight, 200/6743)</strong></font> <br>
                                            <!--<font color="red"><strong>..</strong></font><br>-->
                                            <a href="https://arxiv.org/abs/1906.01140">arXiv</a>/
                                            <a href="https://www.reddit.com/r/MachineLearning/comments/bx8jhz/r_new_sota_for_3d_object_detection/">Reddit</a>/
                                            <a href="https://www.youtube.com/watch?v=Bk727Ec10Ao">Demo</a>/
                                            <font color="blue"> News:</font>
                                            <a href="https://mp.weixin.qq.com/s/jHbWf_SSZE_J6NRJR-96sQ"><font
                                                    color="blue">(新智元,</font></a>
                                            <a href="https://mp.weixin.qq.com/s/wViZITtsb4j3oFtOpJI9wQ"><font
                                                    color="blue">将门创投,</font></a>
                                            <a href="https://mp.weixin.qq.com/s/gybhVw3D4ykAHsVGzazWNw"><font
                                                    color="blue">泡泡机器人)</font>/</a>
                                            <a href="https://github.com/Yang7879/3D-BoNet"><font color="red">Code</font></a>
                                            <iframe src="https://ghbtns.com/github-btn.html?user=Yang7879&repo=3D-BoNet&type=star&count=true&size=small"
                                                    frameborder="0" scrolling="0" width="120px" height="20px"></iframe>

                                        <p align="justify" style="font-size:13px">We propose a simple and efficient
                                            neural
                                            architecture for accurate 3D instance segmentation on point clouds.
                                            It achieves the SOTA performance on ScanNet and S3DIS (June 2019).</p>
                                        <p></p>
                                    </td>
                                </tr>

                                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                                    <tbody>
                                    <tr>
                                        <td width="20%"><img src="./imgs/BMVC2020.png" alt="PontTuset"
                                                             width="180"
                                                             style="border-style: none"></td>
                                        <td width="80%" valign="top">
                                            <p><a href="https://arxiv.org/abs/2008.02312">
                                                <papertitle>Axiom-based Grad-CAM: Towards Accurate Visualization and
                                                    Explanation
                                                    of CNNs
                                                </papertitle>
                                            </a>
                                                <br>R. Fu, <strong>Q. Hu</strong>, X. Dong, Y. Guo, Y. Gao, B. Li<br>
                                                <em>British Machine Vision Conference (BMVC)</em>, 2020 <font
                                                        color="red"><strong>(Oral, 30/670)</strong></font> <br>
                                                <!--<font color="red"><strong>..</strong></font><br>-->
                                                <a href="https://arxiv.org/abs/2008.02312">arXiv</a>/
                                                <a href="https://github.com/Fu0511/XGrad-CAM"><font
                                                        color="red">Code</font></a>
                                                <iframe src="https://ghbtns.com/github-btn.html?user=Fu0511&repo=XGrad-CAM&type=star&count=true&size=small"
                                                        frameborder="0" scrolling="0" width="120px"
                                                        height="20px"></iframe>

                                            <p align="justify" style="font-size:13px"> We introduce two axioms --
                                                Conservation and Sensitivity -- to the visualization paradigm of the CAM
                                                methods. Meanwhile, a dedicated Axiom-based Grad-CAM (XGrad-CAM) is
                                                proposed
                                                to
                                                satisfy these axioms as much as possible. Experiments demonstrate that
                                                XGrad-CAM
                                                is able to achieve better visualization performance than Grad-CAM, while
                                                also be
                                                class-discriminative and easy-to-implement compared with Grad-CAM++ and
                                                Ablation-CAM.</p>
                                            <p></p>
                                        </td>
                                    </tr>


                                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                                        <tbody>
                                        <tr>
                                            <td width="20%"><img src="./imgs/cGAN-Net.png" alt="PontTuset"
                                                                 width="180" style="border-style: none"></td>
                                            <td width="80%" valign="top">
                                                <p><a href="https://arxiv.org/abs/2008.02312">
                                                    <papertitle>CGAN-Net: Class-Guided Asymmetric Non-local Network for
                                                        Real-time Semantic Segmentation
                                                    </papertitle>
                                                </a>
                                                    <br>H. Chen, <strong>Q. Hu</strong>, J. Yang, J. Wu, Y. Guo<br>
                                                    <em>International Conference on Acoustics, Speech, & Signal
                                                        Processing (ICASSP)</em>, 2021 <br>
                                                    <!--<font color="red"><strong>..</strong></font><br>-->

                                                <p align="justify" style="font-size:13px"> we introduce a
                                                    Class-Guided Asymmetric Non-local Network (CGAN-Net) to
                                                    enhance the class-discriminability in learned feature map, while
                                                    maintaining real-time efficiency. The key to our approach is to
                                                    calculate the dense similarity matrix in coarse semantic prediction
                                                    maps, instead of the high-dimensional latent feature map.</p>
                                                <p></p>
                                            </td>
                                        </tr>


                                        <td width="20%"><img src="./imgs/19_TIM.png" alt="PontTuset" width="180"
                                                             style="border-style: none">
                                        </td>
                                        <td width="80%" valign="top">
                                            <p><a href="https://ieeexplore.ieee.org/document/8708248">
                                                <papertitle>Robust Long-term Tracking via Instance Specific Proposals
                                                </papertitle>
                                            </a>
                                                <br>H. Liu, <strong>Q. Hu</strong>, B. Li, Y. Guo<br>
                                                <em>IEEE Transactions on Instrumentation and Measurement (IEEE TIM)</em>,
                                                2019
                                                <strong>(IF=3.06)</strong><br>
                                            </p>
                                            <p></p>
                                            <p align="justify" style="font-size:13px">We propose an efficient and robust
                                                tracker
                                                for
                                                long-term object tracking, which is based on instance specific
                                                proposals. In
                                                particular,
                                                an instance-specific proposal generator is embedded into the error
                                                correction
                                                module
                                                to
                                                recover lost target from tracking failures. </p>
                                        </td>
                                        </tr>


                                        <td width="20%"><img src="./imgs/18_SPL.png" alt="PontTuset" width="180"
                                                             style="border-style: none">
                                        </td>
                                        <td width="80%" valign="top">
                                            <p><a href="https://ieeexplore.ieee.org/document/8474304">
                                                <papertitle>Semi-Online Multiple Object Tracking Using Graphical
                                                    Tracklet
                                                    Association
                                                </papertitle>
                                            </a>
                                                <br>J. Wang, Y. Guo, X. Tang, <strong>Q. Hu</strong>, W. An<br>
                                                <em>IEEE Signal Processing Letters (IEEE SPL)</em>, 2018
                                                <strong>(IF=3.27)</strong><br>
                                                <a href="https://www.youtube.com/watch?v=Wfsu8NVoPJs">Demo</a>
                                            </p>
                                            <p></p>
                                            <p align="justify" style="font-size:13px"> We propose a semi-online MOT
                                                method
                                                using
                                                online
                                                discriminative appearance learning and tracklet association with a
                                                sliding
                                                window.
                                                We
                                                connect similar detections of neighboring frames in a temporal window,
                                                and
                                                improve
                                                the
                                                performance of appearance feature by online discriminative appearance
                                                learning.
                                                Then,
                                                tracklet association is performed by minimizing a subgraph decomposition
                                                cost.</p>
                                        </td>
                                        </tr>

                                        <td width="20%"><img src="./imgs/2018_TIM.png" alt="PontTuset" width="180"
                                                             style="border-style: none"></td>
                                        <td width="80%" valign="top">
                                            <p><a href="https://ieeexplore.ieee.org/document/8012414">
                                                <papertitle>Object tracking using multiple features and adaptive model
                                                    updating
                                                </papertitle>
                                            </a>
                                                <br><strong>Q. Hu</strong>, Y. Guo, Z. Lin, W. An, H. Cheng<br>
                                                <em>IEEE Transactions on Instrumentation and Measurement (IEEE TIM)</em>,
                                                2018
                                                <strong>(IF=3.06)</strong><br>
                                            </p>
                                            <p></p>
                                            <p align="justify" style="font-size:13px">We proposea to combine a 2-D
                                                location
                                                filter
                                                with
                                                a 1-D scale filter to jointly estimate the state of object under
                                                tracking,
                                                and
                                                three
                                                complementary features are integrated to further enhance the overall
                                                tracking
                                                performance. A penalty factor is also defined to achieve a balance
                                                between
                                                stability and flexibility, especially when the object is under
                                                occlusion.
                                            </p>
                                        </td>
                                        </tr>


                                        <td width="20%"><img src="./imgs/18_ICPR.png" alt="PontTuset" width="180"
                                                             style="border-style: none"></td>
                                        <td width="80%" valign="top">
                                            <p><a href="https://ieeexplore.ieee.org/document/8545780">
                                                <papertitle>Long-term Object Tracking with Instance Specific Proposals
                                                </papertitle>
                                            </a>
                                                <br>H. Liu, <strong>Q. Hu</strong>, B. Li, Y. Guo<br>
                                                <em>24th International Conference on Pattern Recognition (ICPR)</em>,
                                                2018<br>
                                            </p>
                                            <p></p>
                                            <p align="justify" style="font-size:13px">We propose a tracker named
                                                Complementary
                                                Learners with Instance-specific Proposals (CLIP). The CLIP tracker
                                                consists
                                                of a
                                                translation filter, a scale filter, and an error correction module. The
                                                error
                                                correction
                                                module is activated to correct the localization error by an
                                                instance-specific
                                                proposal
                                                generator, especially when the target suffers from dramatic appearance
                                                changes.</p>
                                        </td>
                                        </tr>


                                        <td width="20%"><img src="./imgs/17_BMVC.png" alt="PontTuset" width="180"
                                                             style="border-style: none"></td>
                                        <td width="80%" valign="top">
                                            <p><a href="https://arxiv.org/abs/1802.00411">
                                                <papertitle>Correlation Filter Tracking: Beyond an Open-loop System
                                                </papertitle>
                                            </a>
                                                <br><strong>Q. Hu</strong>, Y. Guo, Y. Chen, J. Xiao, W. An<br>
                                                <em>British Machine Vision Conference (BMVC)</em>, 2017 <br>
                                                <a href="https://www.youtube.com/watch?v=dN9L1O_9ZB0">Demo</a> /
                                                <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:ZKBm6H6GuZ0J:scholar.google.com/&output=citation&scisdr=CgXbiqGAEPeF4LGz-rw:AAGBfm0AAAAAXkG24rx8mDZeQvERuq5dFujB1zRmoq3r&scisig=AAGBfm0AAAAAXkG24jggLsN29RWI7IaH4jKtwLHgmh8i&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>
                                                /
                                                <a href="https://github.com/QingyongHu/Correlation-Filter-Tracking-Beyond-an-Open-loop-System"><font
                                                        color="red">Code</font> </a>
                                            </p>
                                            <p></p>
                                            <p align="justify" style="font-size:13px">We interpret object tracking as a
                                                closed-loop
                                                tracking problem, and add a feedback loop to the tracking process by
                                                introducing
                                                an
                                                efficient method to estimate the localization error. We propose a
                                                generic
                                                self-correction mechanism for CF based trackers by introducing a
                                                closed-loop
                                                feedback
                                                technique.</p>
                                        </td>
                                        </tr>


                                        <!--                        <td width="20%"><img src="./imgs/Access_2017.png" alt="this slowpoke moves" width="180"-->
                                        <!--                                             style="border-style: none"></td>-->
                                        <!--                        <td width="80%" valign="top">-->
                                        <!--                            <p><a href="https://ieeexplore.ieee.org/document/7895120">-->
                                        <!--                                <papertitle>Fast correlation tracking using low-dimensional scale filter and local-->
                                        <!--                                    search strategy-->
                                        <!--                                </papertitle>-->
                                        <!--                            </a>-->
                                        <!--                                <br><strong>Q. Hu</strong>, Y. Guo, Z. Lin, X. Deng, W. An<br>-->
                                        <!--                                <em>IEEE Access</em>, 2017 <strong>(IF=4.1)</strong><br>-->
                                        <!--                            </p>-->
                                        <!--                            <p></p>-->
                                        <!--                            <p align="justify" style="font-size:13px">An independent scale filter is proposed to-->
                                        <!--                                estimate the scale of an object, and the dimensionality reduction strategy is used to-->
                                        <!--                                reduce computational cost. In addition, a local search strategy is proposed to expand-->
                                        <!--                                the searching area of the tracker, which can effectively solve the problem caused by-->
                                        <!--                                fast motion and occlusion.</p>-->
                                        <!--                        </td>-->
                                        <!--                        </tr>-->


                                        <td width="20%"><img src="./imgs/2016_DICTA.png" alt="PontTuset" width="180"
                                                             style="border-style: none"></td>
                                        <td width="80%" valign="top">
                                            <p><a href="https://ieeexplore.ieee.org/document/7797075">
                                                <papertitle>Robust and real-time object tracking using scale-adaptive
                                                    correlation
                                                    filters
                                                </papertitle>
                                            </a>
                                                <br><strong>Q. Hu</strong>, Y. Guo, Z. Lin, X. Deng, W. An<br>
                                                <em>Digital Image Computing: Techniques and Applications
                                                    (DICTA)</em>, 2016 </em><font
                                                        color="black"><strong>(Oral)</strong></font><br>
                                            </p>
                                            <p></p>
                                            <p align="justify" style="font-size:13px">We represent the target in kernel
                                                feature
                                                space
                                                and train a classifier on a scale pyramid to achieve adaptive scale
                                                estimation.
                                                We
                                                then
                                                integrate three complementary features to further enhance the overall
                                                tracking
                                                performance..</p>
                                        </td>
                                        </tr>

                                        </tbody>
                                    </table>


                                    <!--SECTION 6 -->
                                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                                        <tbody>
                                        <tr>
                                            <td>
                                                <heading>Teaching</heading>
                                                <p><strong>Trinity Term, 2019</strong>: &ensp;&ensp; <a
                                                        href="https://www.cs.ox.ac.uk/teaching/courses/2019-2020/ai/">
                                                    Artificial
                                                    Intelligence
                                                </a> (University of Oxford). </p>

                                                <p><strong>Hilary Term, 2020</strong>: &ensp;&ensp; <a
                                                        href="https://www.cs.ox.ac.uk/teaching/courses/2019-2020/ai/">
                                                    Artificial
                                                    Intelligence
                                                </a> (University of Oxford). </p>

                                                <p><strong>Trinity Term, 2020</strong>: &ensp;&ensp; <a
                                                        href="https://www.cs.ox.ac.uk/teaching/courses/2019-2020/ml/">
                                                    Machine
                                                    Learning
                                                </a> (University of Oxford). </p>

                                                <p><strong>Trinity Term, 2020</strong>: &ensp;&ensp; <a
                                                        href="https://www.cs.ox.ac.uk/teaching/courses/2019-2020/graphics/">
                                                    Computer
                                                    Graphics
                                                </a> (University of Oxford). </p>

                                            </td>
                                        </tr>
                                        </tbody>
                                    </table>

                                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                                        <tr>
                                            <td width="100%" valign="middle">
                                                <heading>Reviewer Services</heading>
                                                <ul style="list-style-type:disc;">
                                                    <li>
                                                        <a href="http://cvpr2021.thecvf.com/">
                                                            CVPR 2021: IEEE Conference on Computer Vision and Pattern
                                                            Recognition</li>
                                                    <li>
                                                        <a href="http://iccv2021.thecvf.com/home">
                                                            ICCV 2021: International Conference on Computer Vision
                                                    </li>

                                                    <li>
                                                        <a href="http://www.icra2021.org/">
                                                            ICRA 2021: International Conference on Robotics and
                                                            Automation
                                                    </li>
                                                    <li><a href="https://bmvc2020.github.io/"> BMVC 2020: British
                                                        Machine Vision Conference</li>
                                                    <li>
                                                        <a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-multimedia">
                                                            IEEE Transactions on Multimedia
                                                    </li>
                                                    <li>
                                                        <a href="https://www.sciencedirect.com/journal/information-fusion">
                                                            Information Fusion
                                                    </li>
                                                </ul>
                                            </td>
                                        </tr>
                                    </table>


                                    <!--SECTION 9 -->
                                    <div style="clear:both;">
                                        <p align="right"><font size="2">
                                            <script type="text/javascript" id="clustrmaps"
                                                    src="//cdn.clustrmaps.com/map_v2.js?d=liidPLPwwYqJmgAk1lXn75bsZTzJiDg-yWRt9b6CKwc&cl=ffffff&w=a"></script>
                                        </font></p>
                                        <br/>
                                    </div>


                                    <!--SECTION 10 -->
                                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                                        <tbody>
                                        <tr>
                                            <td><br>
                                                <!--<p align="right"><font size="3">Erd&ouml;s = ? </font><br> -->
                                                <p align="right"><font size="2"> Last update: 2020.02.10. <a
                                                        href="http://www.cs.berkeley.edu/~barron/">Thanks.</a></font>
                                                </p>
                                            </td>
                                        </tr>
                                        </tbody>
                                    </table>


                                    </td>
                                    </tr>
                                    </tbody>
                                </table>
</body>
</html>
